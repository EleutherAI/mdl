{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_json(\"../data/CIFAR_editing_results.json\")\n",
    "df[\"loss_matrix_against_source\"] = df[\"loss_matrix_against_source\"].apply(np.array)\n",
    "df[\"loss_matrix_against_target\"] = df[\"loss_matrix_against_target\"].apply(np.array)\n",
    "df[\"top1_matrix_against_source\"] = df[\"top1_matrix_against_source\"].apply(np.array)\n",
    "df[\"top1_matrix_against_target\"] = df[\"top1_matrix_against_target\"].apply(np.array)\n",
    "# average over `seed`\n",
    "mean_df = df.groupby([\"model\", \"editing_mode\"]).mean().reset_index()\n",
    "# standard error over `seed`\n",
    "scalar_df = df.drop(columns=[\"loss_matrix_against_source\", \"loss_matrix_against_target\", \"top1_matrix_against_source\", \"top1_matrix_against_target\"])\n",
    "stderr_df = scalar_df.groupby([\"model\", \"editing_mode\"]).sem().reset_index()\n",
    "\n",
    "summary_df = pd.merge(mean_df, stderr_df, on=[\"model\", \"editing_mode\"], suffixes=(\"_mean\", \"_stderr\"))\n",
    "row = summary_df.iloc[2]\n",
    "\n",
    "# 10 x 10 matrix of losses source -> target\n",
    "loss_mat = row[\"loss_matrix_against_target\"]\n",
    "# 10 x 10 matrix of accuracies source -> target\n",
    "acc_mat = row[\"top1_matrix_against_target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.drop(columns=[\"loss_matrix_against_source\", \"loss_matrix_against_target\", \"top1_matrix_against_source\", \"top1_matrix_against_target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the loss matrix\n",
    "plt.imshow(loss_mat)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Source\")\n",
    "plt.title(f\"Loss for {row['model']} with {row['editing_mode']} editing\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy matrix\n",
    "plt.imshow(acc_mat)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Source\")\n",
    "plt.title(f\"Accuracy for {row['model']} with {row['editing_mode']} editing\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual comparison to non-least squares quadratic concept editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from concept_erasure import QuadraticFitter\n",
    "from concept_editing import get_editor, get_train_test_data\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "download_dir = \"/mnt/ssd-1/alexm/cifar10\"\n",
    "data = CIFAR10(root=download_dir, download=True)\n",
    "images, labels = zip(*data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.stack(list(map(to_tensor, images))) # n x c x w x h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = get_train_test_data(\n",
    "            total_size=None, test_size=1024, flatten=True\n",
    "        )\n",
    "X_train = X_train.double().cpu()\n",
    "Y_train = Y_train.cpu()\n",
    "fitter = QuadraticFitter.fit(X_train, Y_train)\n",
    "optimal_editor = fitter.editor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bar = X_train.mean(dim=0)\n",
    "X_ctr = X_train - X_bar\n",
    "cov_xx = X_ctr.T @ X_ctr / (X_ctr.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_erasure.optimal_transport import psd_sqrt_rsqrt, psd_sqrt\n",
    "def quadratic_edit(im: torch.Tensor, source: int, target: int, optimal=False):\n",
    "    orig_shape = im.shape\n",
    "    im = im.cpu().double().flatten()\n",
    "    if optimal:\n",
    "        return optimal_editor(im.unsqueeze(0), torch.tensor([source]), target).reshape(orig_shape)\n",
    "    else:\n",
    "        P = fitter.sigma_xx[source]\n",
    "        Q = fitter.sigma_xx[target]\n",
    "        _, inv_sqrt_P = psd_sqrt_rsqrt(P)\n",
    "        sqrt_Q = psd_sqrt(Q)\n",
    "        im_ctr = im - fitter.mean_x[source]\n",
    "        return (sqrt_Q @ inv_sqrt_P @ im_ctr + fitter.mean_x[target]).reshape(orig_shape)\n",
    "    \n",
    "def quadratic_erase(im: torch.Tensor, source: int, optimal=False):\n",
    "    orig_shape = im.shape\n",
    "    im = im.cpu().double().flatten()\n",
    "    if optimal:\n",
    "        return fitter.eraser(im.unsqueeze(0), torch.tensor([source])).reshape(orig_shape)\n",
    "    else:\n",
    "        P = fitter.sigma_xx[source]\n",
    "        Q = cov_xx\n",
    "        _, inv_sqrt_P = psd_sqrt_rsqrt(P)\n",
    "        sqrt_Q = psd_sqrt(Q)\n",
    "        im_ctr = im - fitter.mean_x[source]\n",
    "        return (sqrt_Q @ inv_sqrt_P @ im_ctr + X_bar).reshape(orig_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "idx = 2\n",
    "im = X[idx]\n",
    "source = labels[idx]\n",
    "plt.imshow(im.numpy().transpose(1, 2, 0))\n",
    "plt.title(f\"Original\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 2\n",
    "im_edit_suboptimal = quadratic_edit(torch.tensor(im), source, target, optimal=False)\n",
    "im_edit_optimal = quadratic_edit(torch.tensor(im), source, target, optimal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im_edit_suboptimal.numpy().transpose(1, 2, 0))\n",
    "plt.title(\"Naive quadratic edited\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Q-LEACE edited\")\n",
    "plt.imshow(im_edit_optimal.numpy().transpose(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = im_edit_suboptimal - im_edit_optimal\n",
    "plt.imshow(diff.numpy().transpose(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.abs().mean() / im_edit_optimal.abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_erased_suboptimal = quadratic_erase(torch.tensor(im), source, optimal=False)\n",
    "im_erased_optimal = quadratic_erase(torch.tensor(im), source, optimal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im_erased_suboptimal.numpy().transpose(1, 2, 0))\n",
    "plt.title(\"Naive quadratic erased\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(im_erased_optimal.numpy().transpose(1, 2, 0))\n",
    "plt.title(\"Q-LEACE erased\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im.numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_optimal = im_erased_optimal - im\n",
    "diff_suboptimal = im_erased_suboptimal - im\n",
    "print(diff_optimal.norm().mean() / im.norm().mean())\n",
    "print(diff_suboptimal.norm().mean() / im.norm().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_err_optimal = 0\n",
    "total_err_suboptimal = 0\n",
    "for idx in range(100):\n",
    "    im = X[idx]\n",
    "    source = labels[idx]\n",
    "\n",
    "    im_erased_suboptimal = quadratic_erase(torch.tensor(im), source, optimal=False)\n",
    "    im_erased_optimal = quadratic_erase(torch.tensor(im), source, optimal=True)\n",
    "\n",
    "    diff_optimal = im_erased_optimal - im\n",
    "    diff_suboptimal = im_erased_suboptimal - im\n",
    "    err_subopt = diff_suboptimal.abs().mean() / im.abs().mean()\n",
    "    err_opt = diff_optimal.abs().mean() / im.abs().mean()\n",
    "    total_err_optimal += err_opt\n",
    "    total_err_suboptimal += err_subopt\n",
    "\n",
    "    print(f\"Image {idx}:\")\n",
    "    print(f\"Average error for optimal: {total_err_optimal / (idx + 1)}\")\n",
    "    print(f\"Average error for suboptimal: {total_err_suboptimal / (idx + 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test visionprobe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/alexm/miniconda3/envs/ql/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train+val size: 50000\n",
      "Test size: 1024\n"
     ]
    }
   ],
   "source": [
    "from mdl import VisionProbe\n",
    "from concept_editing import get_train_test_data, evaluate_model\n",
    "import torch\n",
    "device = \"cuda\"\n",
    "NUM_CLASSES = 10\n",
    "X_train, X_test, Y_train, Y_test = get_train_test_data(\n",
    "            train_size=None, test_size=1024, flatten=False, device=device\n",
    "        )\n",
    "\n",
    "model = VisionProbe(\n",
    "            num_classes=NUM_CLASSES,\n",
    "            device=X_train.device,\n",
    "            dtype=torch.float32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   7%|▋         | 7/100 [02:03<27:25, 17.69s/it, loss=2.6] \n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model.fit(X_train, Y_train, max_epochs=100, early_stop_epochs=4, reduce_lr_on_plateau=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_editing import get_editor\n",
    "editor = get_editor(\"linear\", X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 10.46 GiB already allocated; 18.44 MiB free; 10.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/mnt/ssd-1/alexm/mdl/experiments/concept_editing.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B216.153.52.112/mnt/ssd-1/alexm/mdl/experiments/concept_editing.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m evaluate_model(model, X_test, Y_test, editor\u001b[39m=\u001b[39;49meditor)\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/mdl/experiments/concept_editing.py:201\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, X_test, Y_test, editor)\u001b[0m\n\u001b[1;32m    195\u001b[0m X_test \u001b[39m=\u001b[39m (\n\u001b[1;32m    196\u001b[0m     editor(X_test_flat, source_z\u001b[39m=\u001b[39mY_test, target_z\u001b[39m=\u001b[39mY_target)\n\u001b[1;32m    197\u001b[0m     \u001b[39m.\u001b[39mview(X_test\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    198\u001b[0m     \u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    199\u001b[0m )\n\u001b[1;32m    200\u001b[0m Y_test \u001b[39m=\u001b[39m Y_test\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 201\u001b[0m logits \u001b[39m=\u001b[39m get_logits(X_test)\n\u001b[1;32m    202\u001b[0m logits_without_edit \u001b[39m=\u001b[39m get_logits(X_test_original)\n\u001b[1;32m    203\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/mdl/experiments/concept_editing.py:171\u001b[0m, in \u001b[0;36mevaluate_model.<locals>.get_logits\u001b[0;34m(x, batch_size)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_logits\u001b[39m(x, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m):\n\u001b[1;32m    170\u001b[0m     x_batches \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39msplit(batch_size)\n\u001b[0;32m--> 171\u001b[0m     logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([model(x_batch) \u001b[39mfor\u001b[39;49;00m x_batch \u001b[39min\u001b[39;49;00m x_batches])\n\u001b[1;32m    172\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/mdl/experiments/concept_editing.py:171\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_logits\u001b[39m(x, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m):\n\u001b[1;32m    170\u001b[0m     x_batches \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39msplit(batch_size)\n\u001b[0;32m--> 171\u001b[0m     logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([model(x_batch) \u001b[39mfor\u001b[39;00m x_batch \u001b[39min\u001b[39;00m x_batches])\n\u001b[1;32m    172\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/miniconda3/envs/ql/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/mdl/mdl/vision_probe.py:73\u001b[0m, in \u001b[0;36mVisionProbe.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(x))\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/miniconda3/envs/ql/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/miniconda3/envs/ql/lib/python3.11/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/miniconda3/envs/ql/lib/python3.11/site-packages/torchvision/models/resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[1;32m    273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/miniconda3/envs/ql/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/miniconda3/envs/ql/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/miniconda3/envs/ql/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/miniconda3/envs/ql/lib/python3.11/site-packages/torchvision/models/resnet.py:96\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m---> 96\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(out)\n\u001b[1;32m     97\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(out)\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/miniconda3/envs/ql/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/miniconda3/envs/ql/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/mnt/ssd-1/alexm/miniconda3/envs/ql/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 10.46 GiB already allocated; 18.44 MiB free; 10.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, X_test, Y_test, editor=editor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
